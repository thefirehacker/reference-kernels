# name: gemm-rs

files:
  - {"name": "submission.py", "source": "@SUBMISSION@"}
  - {"name": "task.py", "source": "task.py"}
  - {"name": "utils.py", "source": "../utils.py"}
  - {"name": "reference.py", "source": "reference.py"}
  - {"name": "eval.py", "source": "../eval.py"}

lang: "py"

description: |
  Implement a Gemm-ReduceScatter kernel for efficient transformer models
  on a single MI300X device.

  ReduceScatter-Gemm (RS-Gemm) is a technique that combines the ReduceScatter
  communication pattern with General Matrix Multiplication (GEMM) to optimize
  the performance of transformer models on GPUs. It is particularly useful for
  handling large models that exceed the memory capacity of a single GPU by
  distributing the model across multiple GPUs and efficiently scattering the
  results of matrix multiplications.

  Your task:
  - Implement the Gemm-RS kernel to perform matrix multiplications in a
    distributed manner, leveraging the ReduceScatter operation to distribute
    data across multiple GPUs.
  - Ensure that the implementation is optimized for the MI300X architecture,
    taking advantage of its specific hardware features for maximum performance.

  Input:
  - `data`: Tuple of (input: torch.Tensor, weights: torch.Tensor, transposed_weight: bool,
            bias: Optional, None or torch.Tensor, TP_GROUP: group object)
    - input: Local input tensor of shape [M, local_K].
    - weight: Weight tensor of shape [N, local_K] or [local_K, N] if transed_weight is True.
    - transposed_weight: Whether the weight is transposed.
    - bias: bias tensor of shape [N] or None.
    - TP_GROUP: Process group for tensor parallelism

  Output:
  - Tuple containing:
    - output: Resulting tensor of shape [M // world_size, N]

config:
  main: "eval.py"

templates:
  Python: "submission.py"

ranking_by: "geom"

tests:
  - {"world_size": 8, "m": 8192, "n": 3584, "k": 14336, "seed": 42}
  - {"world_size": 8, "m": 8192, "n": 4096, "k": 12288, "seed": 6635}
  - {"world_size": 8, "m": 8192, "n": 4608, "k": 36864, "seed": 4422}
  - {"world_size": 8, "m": 8192, "n": 8192, "k": 28672, "seed": 1536}


benchmarks:
  - {"world_size": 8, "m": 8192, "n": 4096, "k": 14336, "seed": 7168}
  - {"world_size": 8, "m": 8192, "n": 8192, "k": 29568, "seed": 1024}
  - {"world_size": 8, "m": 8192, "n": 8192, "k": 30720, "seed": 2035}
